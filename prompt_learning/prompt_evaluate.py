# 提示学习训练和模型评估
import time
import os
from transformers import BertConfig, BertModel, BertTokenizer, get_cosine_schedule_with_warmup, \
    BertForMaskedLM
import pandas as pd
import numpy as np
import torch, gc
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


class ModelConfig:
    def __init__(self):
        self.project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

        # ========== api 数据集相关配置
        self.dataset_dir = os.path.join(self.project_dir, 'data', 'test_not_sampled_data')
        self.pretrained_model_dir = os.path.join(self.project_dir, "bert-base-uncased")
        self.test_file_path = os.path.join(self.dataset_dir, 'test_data_texts.txt')
        self.test_label_path = os.path.join(self.dataset_dir, 'test_data_labels.csv')
        self.data_name = 'prompt'

        # 如果需要切换数据集，只需要更改上面的配置即可

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_save_dir = os.path.join(self.project_dir, 'cache', 'type4', 'long_template_8')
        self.logs_save_dir = os.path.join(self.project_dir, 'logs')
        self.model_save_path = os.path.join(self.model_save_dir, f'type4_loss_{self.data_name}.bin')
        self.model_acc_save_path = os.path.join(self.model_save_dir, f'type4_acc_{self.data_name}.bin')
        self.log_file_name = os.path.join(self.model_save_dir, 'training_log.txt')
        self.writer = SummaryWriter(f"runs/{self.data_name}")
        self.is_sample_shuffle = True
        self.use_embedding_weight = True
        self.batch_size = 16
        self.max_sen_len = 512  # 为None时则采用每个batch中最长的样本对该batch中的样本进行padding
        self.pad_index = 0
        self.random_state = 2022
        self.learning_rate = 4e-5
        self.weight_decay = 0.1
        self.masked_rate = 0.15
        self.masked_token_rate = 0.8
        self.masked_token_unchanged_rate = 0.5
        self.MASK_POS = None  # 提示模版中 [mask] 位置,需要通过计算后更改

        # tokenizer
        self.tokenizer = BertTokenizer.from_pretrained(self.pretrained_model_dir, local_files_only=True)

        self.use_torch_multi_head = False  # False表示使用model/BasicBert/MyTransformer中的多头实现
        self.epochs = 40
        self.model_val_per_epoch = 1

        if not os.path.exists(self.model_save_dir):
            os.makedirs(self.model_save_dir)
        bert_config_path = os.path.join(self.pretrained_model_dir, "config.json")
        bert_config = BertConfig.from_json_file(bert_config_path)
        for key, value in bert_config.__dict__.items():
            self.__dict__[key] = value


# Pandas显示选项，以便在打印数据框时显示所有列和行。
pd.options.display.max_columns = None
pd.options.display.max_rows = None

# 前缀提示模版
prefix = "The API call combination regqueryvalueexw and regclosekey is most commonly used by Adware. Worms often rely on employing the getfileattributesw API call. Downloader utilize distinct API calls like createdirectoryexw and copyfileexw. Virus commonly rely on the ntdelayexecution API function. Check the API call sequence for the category [MASK] malware."
prefix1 = prefix.replace('.', '. [SEP]')


# 查找一下mask的位置
def find_mask_position(input_text, tokenizer):
    # 将文本内容编码成模型输入所需的格式，包括 token ID、token类型ID 和 attention mask。
    encode_dict = tokenizer.encode_plus(input_text,
                                        max_length=model_config.max_sen_len,
                                        padding="max_length",
                                        truncation=True)

    input_id = encode_dict['input_ids']
    mask_position = None
    for i, token in enumerate(input_id):
        if token == 103:
            mask_position = i
    if mask_position:
        print(f'mask position:{mask_position}')
        return mask_position
    else:
        print(f'mask position not found')
        return None


# 构建数据集


class MyDataSet(Dataset):
    # 输入句子，掩码，类型，标签
    def __init__(self, sen, mask, typ, label):
        super(MyDataSet, self).__init__()
        self.sen = torch.tensor(sen, dtype=torch.long)
        self.mask = torch.tensor(mask, dtype=torch.long)
        self.typ = torch.tensor(typ, dtype=torch.long)
        self.label = torch.tensor(label, dtype=torch.long)

    def __len__(self):
        return self.sen.shape[0]

    def __getitem__(self, idx):
        return self.sen[idx], self.mask[idx], self.typ[idx], self.label[idx]


def define_labels():
    labels = {'trojan': 0,
              'backdoor': 1,
              'downloader': 2,
              'worms': 3,
              'spyware': 4,
              'adware': 5,
              'dropper': 6,
              'virus': 7
              }
    label_ids = {}
    tokenizer = BertTokenizer.from_pretrained(model_config.pretrained_model_dir, local_files_only=True)
    for label, index in labels.items():
        label_ids[label] = tokenizer.convert_tokens_to_ids(label)
    return label_ids


class Bert_Model(nn.Module):
    def __init__(self, bert_path):
        super(Bert_Model, self).__init__()
        self.bert = BertForMaskedLM.from_pretrained(bert_path)  # 加载预训练模型权重

    def forward(self, input_ids, attention_mask, token_type_ids):
        outputs = self.bert(input_ids, attention_mask,
                            token_type_ids)  # masked LM 输出的是 mask的值 对应的ids的概率 ，输出 会是词表大小，里面是概率
        logit = outputs[0]  # 包含预测的 token 的概率分布

        return logit


# load  data

# 读取数据并组合成数组
def combine_files(API_path, label_path):
    # 标签与数字的对应关系
    label_dict = {'Trojan': 0, 'Backdoor': 1, 'Downloader': 2, 'Worms': 3, 'Spyware': 4, 'Adware': 5, 'Dropper': 6,
                  'Virus': 7}

    # 初始化空列表，用于存储text和label
    texts = []
    labels = []

    # 读取第一个文件
    with open(API_path, 'r') as API_file:
        for line in API_file:
            # 去除每行末尾的换行符
            line = line.rstrip()
            # 将处理后的行添加到texts列表
            texts.append(line)

    # 读取第二个文件
    with open(label_path, 'r') as label_file:
        for line in label_file:
            # 去除每行末尾的换行符
            line = line.rstrip()
            # 使用标签字典将标签转换为数字，并将处理后的行添加到labels列表
            label = label_dict.get(line)
            if label is not None:
                labels.append(label)
            else:
                print(f"Error: Label '{line}' not found in the dictionary.")
                return None

    # 确保两个列表长度一致
    if len(texts) == len(labels):

        return texts, labels
    else:
        print("Error: Lengths of texts and labels do not match.")
        return None


# 对每个序列，拆分成400一段的子序列。再分别进行数据处理
def ProcessData(API_path, label_path):
    x_train, y_train = combine_files(API_path, label_path)
    # 将API序列转换为API函数的列表形式

    Labelid = []
    Inputid = []
    Typeid = []
    Attenmask = []

    for i in range(len(x_train)):
        sub_sequences = []

        api_sequence = x_train[i].split()
        # 根据API函数的个数确定子序列的长度
        sub_sequence_length = len(api_sequence) // 400  # 取整
        if len(api_sequence) % 400 != 0:  # 如果API函数个数不能被400整除，子序列长度加1
            sub_sequence_length += 1

        # 将API序列分割成子序列
        for k in range(sub_sequence_length):
            sub_sequence = api_sequence[k * 400: (k + 1) * 400]
            sub_sequences.append(sub_sequence)

        inputid, labelid, typeid, attenmask = Process_sub_Data(sub_sequences, y_train[i])

        Labelid.append(labelid)
        Inputid.append(inputid)
        Typeid.append(typeid)
        Attenmask.append(attenmask)

    return Inputid, Labelid, Typeid, Attenmask


# 对子序列进行数据处理
def Process_sub_Data(sub_sequences, label):
    text = []
    Inputid = []
    Labelid = []
    typeid = []
    attenmask = []
    label_ids = define_labels()
    MASK_POS = model_config.MASK_POS

    for i in range(len(sub_sequences)):
        # 在每个句子中的每个句号后添加[SEP]
        text_ = prefix1 + ' '.join(sub_sequences[i])  # 使用join方法将列表转换为字符串
        # 将文本内容编码成模型输入所需的格式，包括 token ID、token类型ID 和 attention mask。
        encode_dict = model_config.tokenizer.encode_plus(text_,
                                                         max_length=model_config.max_sen_len,
                                                         padding="max_length",
                                                         truncation=True)

        input_ids = encode_dict["input_ids"]
        type_ids = encode_dict["token_type_ids"]
        atten_mask = encode_dict["attention_mask"]
        # 对列表进行复制
        labelid, inputid = input_ids[:], input_ids[:]

        if label == 0:
            labelid[MASK_POS] = label_ids['trojan']
            continue

        elif label == 1:
            labelid[MASK_POS] = label_ids['backdoor']
            continue

        elif label == 2:
            labelid[MASK_POS] = label_ids['downloader']
        elif label == 3:
            labelid[MASK_POS] = label_ids['worms']

        elif label == 4:
            labelid[MASK_POS] = label_ids['spyware']
            continue

        elif label == 5:
            labelid[MASK_POS] = label_ids['adware']

        elif label == 6:
            labelid[MASK_POS] = label_ids['dropper']
            continue

        elif label == 7:
            labelid[MASK_POS] = label_ids['virus']

        # 除了mask对应的位置是病毒家族的token id，其他都是-1
        labelid[:MASK_POS] = [-1] * len(labelid[:MASK_POS])
        labelid[MASK_POS + 1:] = [-1] * len(labelid[MASK_POS + 1:])
        inputid[MASK_POS] = model_config.tokenizer.mask_token_id

        text.append(text_)
        Labelid.append(labelid)
        Inputid.append(inputid)
        typeid.append(type_ids)
        attenmask.append(atten_mask)

    return Inputid, Labelid, typeid, attenmask


def print_and_log(message, file):
    print(message)
    print('正在写入日志文件：')
    with open(file, 'a') as log_file:
        log_file.write(message + '\n')


def evaluate_model_performance(config):
    # 不训练，直接评估模型在测试集上的性能。
    model = Bert_Model(bert_path=config.pretrained_model_dir)

    # 查看本地是否存在相关模型（否则无法评估）
    if os.path.exists(config.model_acc_save_path):
        model.load_state_dict(torch.load(config.model_acc_save_path))
        print("从以下路径加载模型参数：", config.model_acc_save_path)
    else:
        print("未找到保存的参数。无法进行评估测试")

    model = model.to(config.device)
    # 模型评估
    model.eval()
    Inputid_test, Labelid_test, typeids_test, inputnmask_test = ProcessData(config.test_file_path,
                                                                            config.test_label_path)

    predictions = []
    true_labels = []
    MASK_POS = config.MASK_POS

    with torch.no_grad():
        # 遍历每一条序列
        for k in range(len(Inputid_test)):

            batch_predictions = []

            # 将当前序列加载到DataLoader中，以16为一个批次（多了容易out of memory)
            test_dataset = DataLoader(MyDataSet(Inputid_test[k], inputnmask_test[k], typeids_test[k], Labelid_test[k]),
                                      16,
                                      True)

            # 遍历当前 API 序列中的每个子序列
            for input_ids, attention_mask, type_ids, test_labels_processed in test_dataset:
                input_ids, attention_mask, type_ids, test_labels_processed = input_ids.to(config.device), \
                                                                             attention_mask.to(config.device), \
                                                                             type_ids.to(config.device), \
                                                                             test_labels_processed.to(config.device)
                outputs = model(input_ids, attention_mask, type_ids)
                predicted_label = torch.argmax(outputs[:, MASK_POS, :], dim=1)
                batch_predictions.append(predicted_label)

            # 将每个子序列的预测结果合并成整个 API 序列的预测结果
            api_sequence_pred = {}

            if batch_predictions:
                for labels in batch_predictions:
                    for label in labels:
                        label_scalar = label.item()  # 将张量转换为标量
                        if label_scalar in api_sequence_pred:
                            api_sequence_pred[label_scalar] += 1
                        else:
                            api_sequence_pred[label_scalar] = 1
                # 选取占比最大的类别作为整个 API 序列的预测结果

                final_pred = max(api_sequence_pred, key=api_sequence_pred.get)
                final_pred_numpy = np.array(final_pred)  # 将预测结果转换为NumPy数组

            # 选取占比最大的类别作为整个 API 序列的预测结果

            predictions.append(final_pred_numpy)

            # 将当前 API 序列的真实标签添加到总真实标签列表中
            true_labels.append(test_labels_processed[0][MASK_POS].cpu().numpy())

    # 计算指标
    accuracy = accuracy_score(true_labels, predictions)
    precision = precision_score(true_labels, predictions, average='weighted')
    recall = recall_score(true_labels, predictions, average='weighted')
    f1 = f1_score(true_labels, predictions, average='weighted')

    message = (
        f'accuracy: {accuracy:.4f}, precision: {precision:.4f}, '
        f'recall: {recall:.4f}, f1: {f1:.4f}, '
    )
    print(message)

    return accuracy, precision, recall, f1


model_config = ModelConfig()

if __name__ == '__main__':
    model_config.MASK_POS = find_mask_position(prefix1, model_config.tokenizer)
    evaluate_model_performance(model_config)
