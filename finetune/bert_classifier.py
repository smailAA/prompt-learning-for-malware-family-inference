# 在bert上简单加一个分类层进行训练
# 通过微调进行恶意家族分类


from transformers import BertTokenizer
from transformers import BertModel, BertConfig
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import os

# Numerical Operations
import math
import numpy as np

# Reading/Writing Data


# For Progress Bar
from tqdm import tqdm

# Pytorch
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
from torch.optim import Adam

# For plotting learning curve
from torch.utils.tensorboard import SummaryWriter


# 将8个病毒家族类进行赋值
labels = {'trojan': 0,
          'backdoor': 1,
          'downloader': 2,
          'worms': 3,
          'spyware': 4,
          'adware': 5,
          'dropper': 6,
          'virus': 7
          }


class ModelConfig:
    def __init__(self):
        self.project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

        # ========== api 数据集相关配置
        self.dataset_dir = os.path.join(self.project_dir, 'data', 'type4_classification')
        self.pretrained_model_dir = os.path.join(self.project_dir, "bert-base-uncased")
        self.train_file_path = os.path.join(self.dataset_dir, 'train_data_texts.txt')
        self.val_file_path = os.path.join(self.dataset_dir, 'dev_data_texts.txt')
        self.test_file_path = os.path.join(self.dataset_dir, 'test_data_texts.txt')
        self.train_label_path = os.path.join(self.dataset_dir, 'train_data_labels.csv')
        self.val_label_path = os.path.join(self.dataset_dir, 'dev_data_labels.csv')
        self.test_label_path = os.path.join(self.dataset_dir, 'test_data_labels.csv')
        self.data_name = 'finetune'

        # 如果需要切换数据集，只需要更改上面的配置即可

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_save_dir = os.path.join(self.project_dir, 'cache', 'finetune', 'direct')
        self.logs_save_dir = os.path.join(self.project_dir, 'logs')
        self.model_save_path = os.path.join(self.model_save_dir, f'type4_loss_{self.data_name}.bin')
        self.model_acc_save_path = os.path.join(self.model_save_dir, f'type4_acc_{self.data_name}.bin')
        self.log_file_name = os.path.join(self.model_save_dir, 'training_log.txt')
        self.writer = SummaryWriter(f"runs/{self.data_name}")
        self.batch_size = 30
        self.epochs = 60
        self.learning_rate = 1e-5
        self.weight_decay = 0.1
        self.early_stop = 30

        # tokenizer
        self.tokenizer = BertTokenizer.from_pretrained(self.pretrained_model_dir, local_files_only=True)
        self.use_torch_multi_head = False  # False表示使用model/BasicBert/MyTransformer中的多头实现

        if not os.path.exists(self.model_save_dir):
            os.makedirs(self.model_save_dir)
        bert_config_path = os.path.join(self.pretrained_model_dir, "config.json")
        bert_config = BertConfig.from_json_file(bert_config_path)
        for key, value in bert_config.__dict__.items():
            self.__dict__[key] = value


class APIDataset(Dataset):
    def __init__(self, result, max_length=512):
        self.labels = [labels[label] for label, _ in result]
        self.texts = [model_config.tokenizer(text,
                                padding='max_length',
                                max_length=max_length,
                                truncation=True,
                                return_tensors="pt")
                      for label, text in result]

        self.max_length = max_length

    def __len__(self):
        # 返回数据集的长度
        return len(self.labels)

    def get_batch_labels(self, idx):
        # 获取一批标签，返回 PyTorch 张量，指定类型为64位整数
        return torch.tensor(self.labels[idx], dtype=torch.long)

    def get_batch_texts(self, idx):
        # 获取一批 tokenized 文本，返回包含 'input_ids' 和 'attention_mask' 的字典
        return self.texts[idx]

    def __getitem__(self, idx):
        # 获取一批文本和标签，返回一个字典
        batch_texts = self.get_batch_texts(idx)
        batch_y = self.get_batch_labels(idx)
        return batch_texts, batch_y


# 读取数据并组合成数组
def combine_files(API_path, label_path):
    # 初始化空列表，用于存储text和label
    texts = []
    labels = []

    # 读取第一个文件
    with open(API_path, 'r') as API_file:
        for line in API_file:
            # 去除每行末尾的换行符
            line = line.rstrip()
            # 将处理后的行添加到texts列表
            texts.append(line)

    # 读取第二个文件
    with open(label_path, 'r') as label_file:
        for line in label_file:
            # 去除每行末尾的换行符
            line = line.rstrip()
            # 将处理后的行添加到labels列表
            labels.append(line)

    # 确保两个列表长度一致
    if len(texts) == len(labels):
        # 将text和label按顺序组合在一起
        combined_data = list(zip(labels, texts))
        return combined_data
    else:
        print("Error: Lengths of texts and labels do not match.")
        return None


class ImprovedBertClassifier(nn.Module):
    def __init__(self,bert_path, num_classes=8, hidden_layers=1, hidden_dim=256, dropout=0.25):
        super(ImprovedBertClassifier, self).__init__()

        # 神经网络基本构建模块，由一个线性层和一个ReLU组成
        class BasicBlock(nn.Module):
            def __init__(self, input_dim, output_dim, dropout):
                super(BasicBlock, self).__init__()

                self.block = nn.Sequential(
                    nn.Linear(input_dim, output_dim),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                )

            def forward(self, x):
                x = self.block(x)
                return x

        # 分类器模型，利用了多个 BasicBlock 构建了一个具有一定深度的神经网络模型
        class Classifier(nn.Module):
            def __init__(self, input_dim, output_dim, hidden_layers, hidden_dim, dropout):
                super(Classifier, self).__init__()

                self.fc = nn.Sequential(
                    # 输入层
                    BasicBlock(input_dim, hidden_dim, dropout),
                    # 列表生成式，根据指定的隐藏层数量多次堆叠了 BasicBlock，将隐藏层之间相互连接起来。
                    *[BasicBlock(hidden_dim, hidden_dim, dropout) for _ in range(hidden_layers)],
                    # 输出层
                    nn.Linear(hidden_dim, output_dim)
                )

            def forward(self, x):
                x = self.fc(x)
                return x

        # 使用BERT作为特征提取器，加载预训练参数
        self.bert = BertModel.from_pretrained(bert_path)
        self.dropout = nn.Dropout(dropout)

        # 获取BERT模型输出的维度
        bert_output_dim = self.bert.pooler.dense.out_features

        # 创建分类器，输入维度为BERT模型输出维度
        self.classifier = Classifier(bert_output_dim, num_classes, hidden_layers, hidden_dim, dropout)

    def forward(self, input_id, mask):
        # 在上面的代码中命名的第一个变量_包含sequence中所有 token 的 Embedding 向量层。
        # 命名的第二个变量pooled_output包含 [CLS] token 的 Embedding 向量。
        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)
        dropout_output = self.dropout(pooled_output)
        output = self.classifier(dropout_output)
        return output


def trainer(model, train_loader, valid_loader, config):
    log_filename = config.log_file_name
    # 定义损失函数和优化器
    criterion = nn.CrossEntropyLoss()
    optimizer = Adam(model.parameters(), lr=config.learning_rate)
    device = config.device

    # 记录训练过程中的损失值信息
    writer = SummaryWriter()  # Writer of tensoboard.

    # 设定配置好的参数
    n_epochs = config.epochs
    best_loss = math.inf
    best_accuracy = 0
    step = 0
    early_stop_count_accuracy = 0
    early_stop_count_loss = 0

    # 移动到对应的设备上去
    model = model.to(device)
    criterion = criterion.to(device)
    # 开始进入训练循环
    for epoch in range(n_epochs):
        model.train()  # Set your model to train mode.
        # 列表，用于计算平均损失
        loss_record = []
        acc_record = 0

        # 进度条函数tqdm
        # 使用 train_loader 遍历训练数据集
        train_pbar = tqdm(train_loader, position=0, leave=True)
        for train_input, train_label in train_pbar:
            # 梯度缓存归零，用来计算新的梯度
            optimizer.zero_grad()  # Set gradient to zero.
            train_label = train_label.to(device)
            mask = train_input['attention_mask'].to(device)
            input_id = train_input['input_ids'].squeeze(1).to(device)
            # 通过模型得到输出
            output = model(input_id, mask)
            # 计算损失
            batch_loss = criterion(output, train_label)

            # 计算准确率
            predictions = torch.argmax(output, dim=1)
            correct_predictions = (predictions == train_label).sum().item()
            total_predictions = train_label.size(0)
            batch_accuracy = correct_predictions / total_predictions
            acc_record += batch_accuracy

            # 模型更新
            model.zero_grad()
            batch_loss.backward()
            optimizer.step()
            step += 1
            # 将当前 batch 的损失值添加到 loss_record 列表中，用于计算平均训练损失。
            loss_record.append(batch_loss.detach().item())
            # 显示当前epoch和损失值
            train_pbar.set_description(f'Epoch [{epoch + 1}/{n_epochs}]')
            train_pbar.set_postfix({'loss': batch_loss.detach().item()})

        # 计算当前epoch中所有batch的平均损失和平均准确率
        mean_train_loss = sum(loss_record) / len(loss_record)
        mean_train_accuracy = acc_record / len(train_loader)
        # 将平均训练损失写入到 TensorBoard 中，用于训练过程的可视化。
        writer.add_scalar('Loss/train', mean_train_loss, step)
        writer.add_scalar('Accuracy/train', mean_train_accuracy, step)

        # ------ 验证模型 -----------

        model.eval()  # Set your model to evaluation mode.
        loss_record = []
        all_predictions = []
        all_labels = []

        for val_input, val_label in valid_loader:
            val_label = val_label.to(device)
            mask = val_input['attention_mask'].to(device)
            input_id = val_input['input_ids'].squeeze(1).to(device)
            with torch.no_grad():
                output = model(input_id, mask)
                batch_loss = criterion(output, val_label)
                # 将模型的预测转换为类别标签
                predictions = torch.argmax(output, dim=1).cpu().numpy()
                all_predictions.extend(predictions)
                all_labels.extend(val_label.cpu().numpy())

            # 将当前 batch 的损失值添加到 loss_record 列表中，用于计算平均训练损失。
            loss_record.append(batch_loss.detach().item())
        # 计算验证集上的平均损失，表示当前 epoch 下模型在验证集上的性能。
        mean_valid_loss = sum(loss_record) / len(loss_record)
        accuracy = accuracy_score(all_labels, all_predictions)
        precision = precision_score(all_labels, all_predictions, average='weighted')
        recall = recall_score(all_labels, all_predictions, average='weighted')
        f1 = f1_score(all_labels, all_predictions, average='weighted')

        message = (f'Epoch [{epoch + 1}/{n_epochs}]: '
                   f'Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}, '
                   f'Train Accuracy:{mean_train_accuracy:.4f},'
                   f'Valid Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, '
                   f'Recall: {recall:.4f}, F1: {f1:.4f}')
        print_and_log(message, log_filename)

        writer.add_scalar('Loss/valid', mean_valid_loss, step)
        writer.add_scalar('Metrics/accuracy', accuracy, step)
        writer.add_scalar('Metrics/precision', precision, step)
        writer.add_scalar('Metrics/recall', recall, step)
        writer.add_scalar('Metrics/f1', f1, step)

        if mean_valid_loss < best_loss:
            # 如果当前 epoch 的验证损失比历史最低验证损失还低，就更新最低验证损失，并保存模型参数到指定路径。
            best_loss = mean_valid_loss
            torch.save(model.state_dict(), config.model_save_path)  # Save model with lowest loss
            print('Saving model with loss {:.3f}...'.format(best_loss))
            early_stop_count_loss = 0  # Reset early stopping counter for loss
        else:
            early_stop_count_loss += 1

        if accuracy > best_accuracy:
            # 如果当前 epoch 的准确率比历史最高准确率还高，就更新最高准确率，并保存模型参数到指定路径。
            best_accuracy = accuracy
            torch.save(model.state_dict(), config.model_acc_save_path)  # Save model with highest accuracy
            print('Saving model with accuracy {:.4f}...'.format(best_accuracy))
            early_stop_count_accuracy = 0  # Reset early stopping counter for accuracy
        else:
            early_stop_count_accuracy += 1

        # 如果连续 early_stop 次 epoch 验证损失和准确率都没有改善，则提前结束训练。
        if early_stop_count_loss >= config.early_stop and early_stop_count_accuracy >= config.early_stop:
            print('\nModel is not improving, so we halt the training session.')
            return


def print_and_log(message, file):
    print(message)
    print('正在写入日志文件：')
    with open(file, 'a') as log_file:
        log_file.write(message + '\n')


# get the data and label

def ProcessData(API_path, label_path):
    combine_data = combine_files(API_path, label_path)
    return np.array(combine_data)


def train_pro(config):
    # 初始化模型
    model = ImprovedBertClassifier(bert_path=config.pretrained_model_dir,num_classes=8, hidden_layers=8, hidden_dim=256, dropout=0.25)

    # 检查是否有保存的参数并加载它们
    if os.path.exists(config.model_save_path):
        model.load_state_dict(torch.load(config.model_save_path))
        print("从以下路径加载模型参数：", config.model_save_path)
    else:
        print("未找到保存的参数。从头开始训练。")

    train_data = ProcessData(config.train_file_path, config.train_label_path)
    valid_data = ProcessData(config.val_file_path, config.val_label_path)

    # Print out the data size.
    print(f"""train_data size: {train_data.shape} 
            valid_data size: {valid_data.shape}""")

    # 创建对象实例，以tensor格式存储
    # 通过Dataset类获取训练和验证集
    train_dataset, val_dataset = APIDataset(train_data), APIDataset(valid_data)
    # DataLoader根据batch_size获取数据，训练时选择打乱样本
    train_dataloader = DataLoader(train_dataset, config.batch_size, shuffle=True, pin_memory=True)
    val_dataloader = DataLoader(val_dataset, config.batch_size, shuffle=True, pin_memory=True)
    trainer(model, train_dataloader, val_dataloader, config)


def evaluate_model_performance(config):
    # 不训练，直接评估模型在测试集上的性能。
    # 初始化模型
    model = ImprovedBertClassifier(bert_path=config.pretrained_model_dir,num_classes=8, hidden_layers=8, hidden_dim=256, dropout=0.25)
    device = config.device

    # 查看本地是否存在相关模型（否则无法评估）

    if os.path.exists(config.model_acc_save_path):
        model.load_state_dict(torch.load(config.model_acc_save_path))
        print("从以下路径加载模型参数：", config.model_acc_save_path)
    else:
        print("未找到保存的参数。无法进行评估测试")

    model = model.to(config.device)
    # 模型评估
    model.eval()

    test_data = ProcessData(config.test_file_path, config.test_label_path)

    # 创建对象实例，以tensor格式存储
    # 通过Dataset类获取训练和验证集
    test_dataset = APIDataset(test_data)

    # DataLoader根据batch_size获取数据，训练时选择打乱样本
    test_dataloader = DataLoader(test_dataset, config.batch_size, shuffle=True, pin_memory=True)

    # ------ 验证模型 -----------

    all_predictions = []
    all_labels = []

    for test_input, test_label in test_dataloader:
        test_label = test_label.to(device)
        mask = test_input['attention_mask'].to(device)
        input_id = test_input['input_ids'].squeeze(1).to(device)
        with torch.no_grad():
            output = model(input_id, mask)

            # 将模型的预测转换为类别标签
            predictions = torch.argmax(output, dim=1).cpu().numpy()
            all_predictions.extend(predictions)
            all_labels.extend(test_label.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_predictions)
    precision = precision_score(all_labels, all_predictions, average='weighted')
    recall = recall_score(all_labels, all_predictions, average='weighted')
    f1 = f1_score(all_labels, all_predictions, average='weighted')

    message = (f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, '
               f'Recall: {recall:.4f}, F1: {f1:.4f}')

    print(message)

    return accuracy, precision, recall, f1


model_config = ModelConfig()
if __name__ == '__main__':

    evaluate_model_performance(model_config)
