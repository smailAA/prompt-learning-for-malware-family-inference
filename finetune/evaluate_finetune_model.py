# 在bert上简单加一个分类层进行训练
# 通过微调进行恶意家族分类


from transformers import BertTokenizer
from transformers import BertModel, BertConfig
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import os

# Numerical Operations
import math
import numpy as np

# Reading/Writing Data


# For Progress Bar
from tqdm import tqdm

# Pytorch
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
from torch.optim import Adam

# For plotting learning curve
from torch.utils.tensorboard import SummaryWriter


# 将8个病毒家族类进行赋值
labels = {'trojan': 0,
          'backdoor': 1,
          'downloader': 2,
          'worms': 3,
          'spyware': 4,
          'adware': 5,
          'dropper': 6,
          'virus': 7
          }


class ModelConfig:
    def __init__(self):
        self.project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

        # ========== api 数据集相关配置
        self.dataset_dir = os.path.join(self.project_dir, 'data', 'type4_classification')
        self.pretrained_model_dir = os.path.join(self.project_dir, "bert-base-uncased")
        self.train_file_path = os.path.join(self.dataset_dir, 'train_data_texts.txt')
        self.val_file_path = os.path.join(self.dataset_dir, 'dev_data_texts.txt')
        self.test_file_path = os.path.join(self.dataset_dir, 'test_data_texts.txt')
        self.train_label_path = os.path.join(self.dataset_dir, 'train_data_labels.csv')
        self.val_label_path = os.path.join(self.dataset_dir, 'dev_data_labels.csv')
        self.test_label_path = os.path.join(self.dataset_dir, 'test_data_labels.csv')
        self.data_name = 'finetune'

        # 如果需要切换数据集，只需要更改上面的配置即可

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_save_dir = os.path.join(self.project_dir, 'cache', 'finetune', 'api_pretrain')
        self.logs_save_dir = os.path.join(self.project_dir, 'logs')
        self.model_save_path = os.path.join(self.model_save_dir, f'type4_loss_{self.data_name}.bin')
        self.model_acc_save_path = os.path.join(self.model_save_dir, f'type4_acc_{self.data_name}.bin')
        self.log_file_name = os.path.join(self.model_save_dir, 'training_log.txt')
        self.writer = SummaryWriter(f"runs/{self.data_name}")
        self.batch_size = 30
        self.epochs = 60
        self.learning_rate = 1e-5
        self.weight_decay = 0.1
        self.early_stop = 30

        # tokenizer
        self.tokenizer = BertTokenizer.from_pretrained(self.pretrained_model_dir, local_files_only=True)
        self.use_torch_multi_head = False  # False表示使用model/BasicBert/MyTransformer中的多头实现

        if not os.path.exists(self.model_save_dir):
            os.makedirs(self.model_save_dir)
        bert_config_path = os.path.join(self.pretrained_model_dir, "config.json")
        bert_config = BertConfig.from_json_file(bert_config_path)
        for key, value in bert_config.__dict__.items():
            self.__dict__[key] = value


class APIDataset(Dataset):
    def __init__(self, result, max_length=512):
        self.labels = [labels[label] for label, _ in result]
        self.texts = [model_config.tokenizer(text,
                                padding='max_length',
                                max_length=max_length,
                                truncation=True,
                                return_tensors="pt")
                      for label, text in result]

        self.max_length = max_length

    def __len__(self):
        # 返回数据集的长度
        return len(self.labels)

    def get_batch_labels(self, idx):
        # 获取一批标签，返回 PyTorch 张量，指定类型为64位整数
        return torch.tensor(self.labels[idx], dtype=torch.long)

    def get_batch_texts(self, idx):
        # 获取一批 tokenized 文本，返回包含 'input_ids' 和 'attention_mask' 的字典
        return self.texts[idx]

    def __getitem__(self, idx):
        # 获取一批文本和标签，返回一个字典
        batch_texts = self.get_batch_texts(idx)
        batch_y = self.get_batch_labels(idx)
        return batch_texts, batch_y


# 读取数据并组合成数组
def combine_files(API_path, label_path):
    # 初始化空列表，用于存储text和label
    texts = []
    labels = []

    # 读取第一个文件
    with open(API_path, 'r') as API_file:
        for line in API_file:
            # 去除每行末尾的换行符
            line = line.rstrip()
            # 将处理后的行添加到texts列表
            texts.append(line)

    # 读取第二个文件
    with open(label_path, 'r') as label_file:
        for line in label_file:
            # 去除每行末尾的换行符
            line = line.rstrip()
            # 将处理后的行添加到labels列表
            labels.append(line)

    # 确保两个列表长度一致
    if len(texts) == len(labels):
        # 将text和label按顺序组合在一起
        combined_data = list(zip(labels, texts))
        return combined_data
    else:
        print("Error: Lengths of texts and labels do not match.")
        return None


class ImprovedBertClassifier(nn.Module):
    def __init__(self,bert_path, num_classes=8, hidden_layers=1, hidden_dim=256, dropout=0.25):
        super(ImprovedBertClassifier, self).__init__()

        # 神经网络基本构建模块，由一个线性层和一个ReLU组成
        class BasicBlock(nn.Module):
            def __init__(self, input_dim, output_dim, dropout):
                super(BasicBlock, self).__init__()

                self.block = nn.Sequential(
                    nn.Linear(input_dim, output_dim),
                    nn.ReLU(),
                    nn.Dropout(dropout),
                )

            def forward(self, x):
                x = self.block(x)
                return x

        # 分类器模型，利用了多个 BasicBlock 构建了一个具有一定深度的神经网络模型
        class Classifier(nn.Module):
            def __init__(self, input_dim, output_dim, hidden_layers, hidden_dim, dropout):
                super(Classifier, self).__init__()

                self.fc = nn.Sequential(
                    # 输入层
                    BasicBlock(input_dim, hidden_dim, dropout),
                    # 列表生成式，根据指定的隐藏层数量多次堆叠了 BasicBlock，将隐藏层之间相互连接起来。
                    *[BasicBlock(hidden_dim, hidden_dim, dropout) for _ in range(hidden_layers)],
                    # 输出层
                    nn.Linear(hidden_dim, output_dim)
                )

            def forward(self, x):
                x = self.fc(x)
                return x

        # 使用BERT作为特征提取器，加载预训练参数
        self.bert = BertModel.from_pretrained(bert_path)
        self.dropout = nn.Dropout(dropout)

        # 获取BERT模型输出的维度
        bert_output_dim = self.bert.pooler.dense.out_features

        # 创建分类器，输入维度为BERT模型输出维度
        self.classifier = Classifier(bert_output_dim, num_classes, hidden_layers, hidden_dim, dropout)

    def forward(self, input_id, mask):
        # 在上面的代码中命名的第一个变量_包含sequence中所有 token 的 Embedding 向量层。
        # 命名的第二个变量pooled_output包含 [CLS] token 的 Embedding 向量。
        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)
        dropout_output = self.dropout(pooled_output)
        output = self.classifier(dropout_output)
        return output





def print_and_log(message, file):
    print(message)
    print('正在写入日志文件：')
    with open(file, 'a') as log_file:
        log_file.write(message + '\n')


# get the data and label

def ProcessData(API_path, label_path):
    combine_data = combine_files(API_path, label_path)
    return np.array(combine_data)



def evaluate_model_performance(config):
    # 不训练，直接评估模型在测试集上的性能。
    # 初始化模型
    model = ImprovedBertClassifier(bert_path=config.pretrained_model_dir,num_classes=8, hidden_layers=8, hidden_dim=256, dropout=0.25)
    device = config.device

    # 查看本地是否存在相关模型（否则无法评估）

    if os.path.exists(config.model_acc_save_path):
        model.load_state_dict(torch.load(config.model_acc_save_path))
        print("从以下路径加载模型参数：", config.model_acc_save_path)
    else:
        print("未找到保存的参数。无法进行评估测试")

    model = model.to(config.device)
    # 模型评估
    model.eval()

    test_data = ProcessData(config.test_file_path, config.test_label_path)

    # 创建对象实例，以tensor格式存储
    # 通过Dataset类获取训练和验证集
    test_dataset = APIDataset(test_data)

    # DataLoader根据batch_size获取数据，训练时选择打乱样本
    test_dataloader = DataLoader(test_dataset, config.batch_size, shuffle=True, pin_memory=True)

    # ------ 验证模型 -----------

    all_predictions = []
    all_labels = []

    for test_input, test_label in test_dataloader:
        test_label = test_label.to(device)
        mask = test_input['attention_mask'].to(device)
        input_id = test_input['input_ids'].squeeze(1).to(device)
        with torch.no_grad():
            output = model(input_id, mask)

            # 将模型的预测转换为类别标签
            predictions = torch.argmax(output, dim=1).cpu().numpy()
            all_predictions.extend(predictions)
            all_labels.extend(test_label.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_predictions)
    precision = precision_score(all_labels, all_predictions, average='weighted')
    recall = recall_score(all_labels, all_predictions, average='weighted')
    f1 = f1_score(all_labels, all_predictions, average='weighted')

    message = (f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, '
               f'Recall: {recall:.4f}, F1: {f1:.4f}')

    print(message)

    return accuracy, precision, recall, f1


model_config = ModelConfig()
if __name__ == '__main__':

    evaluate_model_performance(model_config)
