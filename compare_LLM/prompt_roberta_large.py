# 提示学习训练和模型评估
import time
import os

from transformers import RobertaConfig, RobertaModel, RobertaTokenizer, RobertaForMaskedLM, \
    get_cosine_schedule_with_warmup
import pandas as pd
import torch, gc
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, random_split
from torch.utils.tensorboard import SummaryWriter
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

gc.collect()
torch.cuda.empty_cache()


class ModelConfig:
    def __init__(self):
        self.project_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

        # ========== api 数据集相关配置
        self.dataset_dir = os.path.join(self.project_dir, 'data', 'type4_classification')
        self.pretrained_model_dir = os.path.join(self.project_dir, "roberta-large")
        self.train_file_path = os.path.join(self.dataset_dir, 'train_data_texts.txt')
        self.val_file_path = os.path.join(self.dataset_dir, 'dev_data_texts.txt')
        self.test_file_path = os.path.join(self.dataset_dir, 'test_data_texts.txt')
        self.train_label_path = os.path.join(self.dataset_dir, 'train_data_labels.csv')
        self.val_label_path = os.path.join(self.dataset_dir, 'dev_data_labels.csv')
        self.test_label_path = os.path.join(self.dataset_dir, 'test_data_labels.csv')
        self.data_name = 'prompt'

        # 如果需要切换数据集，只需要更改上面的配置即可

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model_save_dir = os.path.join(self.project_dir, 'cache', 'compare_LLM', 'roberta_large')
        self.logs_save_dir = os.path.join(self.project_dir, 'logs')
        self.model_save_path = os.path.join(self.model_save_dir, f'type4_loss_{self.data_name}.bin')
        self.model_acc_save_path = os.path.join(self.model_save_dir, f'type4_acc_{self.data_name}.bin')
        self.log_file_name = os.path.join(self.model_save_dir, 'training_log.txt')
        self.writer = SummaryWriter(f"runs/{self.data_name}")
        self.is_sample_shuffle = True
        self.use_embedding_weight = True
        self.batch_size = 8
        self.max_sen_len = 512  # 为None时则采用每个batch中最长的样本对该batch中的样本进行padding
        self.pad_index = 0
        self.random_state = 2022
        self.learning_rate = 4e-5
        self.weight_decay = 0.1
        self.masked_rate = 0.15
        self.masked_token_rate = 0.8
        self.masked_token_unchanged_rate = 0.5
        self.MASK_POS = None  # 提示模版中 [mask] 位置,需要通过计算后更改

        # tokenizer
        self.tokenizer = RobertaTokenizer.from_pretrained(self.pretrained_model_dir, local_files_only=True)

        self.use_torch_multi_head = False  # False表示使用model/BasicBert/MyTransformer中的多头实现
        self.epochs = 40
        self.model_val_per_epoch = 1

        if not os.path.exists(self.model_save_dir):
            os.makedirs(self.model_save_dir)
        bert_config_path = os.path.join(self.pretrained_model_dir, "config.json")
        bert_config = RobertaConfig.from_json_file(bert_config_path)
        for key, value in bert_config.__dict__.items():
            self.__dict__[key] = value


# Pandas显示选项，以便在打印数据框时显示所有列和行。
pd.options.display.max_columns = None
pd.options.display.max_rows = None

# 前缀提示模版
prefix = 'Check the API call sequence for the category <mask> malware.'

prefix1 = prefix.replace('.', '. </s>')


# 查找一下mask的位置
def find_mask_position(input_text, tokenizer):
    # 将文本内容编码成模型输入所需的格式，包括 token ID、token类型ID 和 attention mask。
    encode_dict = tokenizer.encode_plus(input_text,
                                        max_length=model_config.max_sen_len,
                                        padding="max_length",
                                        truncation=True)

    input_id = encode_dict['input_ids']
    mask_position = None
    for i, token in enumerate(input_id):
        if token == tokenizer.mask_token_id:
            mask_position = i

    if mask_position:

        print(f'mask position:{mask_position}')
        return mask_position
    else:
        print(f'mask position not found')
        return None


def define_labels():
    labels = {'trojan': 0,
              'backdoor': 1,
              'downloader': 2,
              'worms': 3,
              'spyware': 4,
              'adware': 5,
              'dropper': 6,
              'virus': 7
              }
    label_ids = {}
    tokenizer = RobertaTokenizer.from_pretrained(model_config.pretrained_model_dir, local_files_only=True)
    for label, index in labels.items():
        label_ids[label] = tokenizer.convert_tokens_to_ids(label)
    return label_ids


class Roberta_Model(nn.Module):
    def __init__(self, roberta_path):
        super(Roberta_Model, self).__init__()
        self.roberta = RobertaForMaskedLM.from_pretrained(roberta_path)  # 加载预训练模型权重

    def forward(self, input_ids, attention_mask):
        outputs = self.roberta(input_ids, attention_mask)
        logit = outputs[0]

        return logit


# 构建数据集


class MyDataSet(Dataset):
    # 输入句子，掩码，类型，标签
    def __init__(self, sen, mask, label):
        super(MyDataSet, self).__init__()
        self.sen = torch.tensor(sen, dtype=torch.long)
        self.mask = torch.tensor(mask, dtype=torch.long)

        self.label = torch.tensor(label, dtype=torch.long)

    def __len__(self):
        return self.sen.shape[0]

    def __getitem__(self, idx):
        return self.sen[idx], self.mask[idx], self.label[idx]


# load  data

# 读取数据并组合成数组
def combine_files(API_path, label_path):
    # 标签与数字的对应关系
    label_dict = {'trojan': 0, 'backdoor': 1, 'downloader': 2, 'worms': 3, 'spyware': 4, 'adware': 5, 'dropper': 6,
                  'virus': 7}

    # 初始化空列表，用于存储text和label
    texts = []
    labels = []

    # 读取第一个文件
    with open(API_path, 'r') as API_file:
        for line in API_file:
            # 去除每行末尾的换行符
            line = line.rstrip()
            # 将处理后的行添加到texts列表
            texts.append(line)

    # 读取第二个文件
    with open(label_path, 'r') as label_file:
        for line in label_file:
            # 去除每行末尾的换行符
            line = line.rstrip()
            # 使用标签字典将标签转换为数字，并将处理后的行添加到labels列表
            label = label_dict.get(line)
            if label is not None:
                labels.append(label)
            else:
                print(f"Error: Label '{line}' not found in the dictionary.")
                return None

    # 确保两个列表长度一致
    if len(texts) == len(labels):

        return texts, labels
    else:
        print("Error: Lengths of texts and labels do not match.")
        return None


# get the data and label

def ProcessData(API_path, label_path):
    x_train, y_train = combine_files(API_path, label_path)
    # x_train,x_test,y_train,y_test=train_test_split(StrongData,StrongLabel,test_size=0.3, random_state=42)

    text = []
    Inputid = []
    Labelid = []

    attenmask = []
    label_ids = define_labels()
    MASK_POS = model_config.MASK_POS

    for i in range(len(x_train)):
        # 在每个句子中的每个句号后添加[SEP]
        text_ = prefix1 + x_train[i]
        # 将文本内容编码成模型输入所需的格式，包括 token ID、token类型ID 和 attention mask。
        encode_dict = model_config.tokenizer.encode_plus(text_,
                                                         max_length=model_config.max_sen_len,
                                                         padding="max_length",
                                                         truncation=True)

        input_ids = encode_dict["input_ids"]
        atten_mask = encode_dict["attention_mask"]
        # 对列表进行复制
        labelid, inputid = input_ids[:], input_ids[:]

        if y_train[i] == 0:
            labelid[MASK_POS] = label_ids['trojan']
            continue

        elif y_train[i] == 1:
            labelid[MASK_POS] = label_ids['backdoor']
            continue

        elif y_train[i] == 2:
            labelid[MASK_POS] = label_ids['downloader']
        elif y_train[i] == 3:
            labelid[MASK_POS] = label_ids['worms']

        elif y_train[i] == 4:
            labelid[MASK_POS] = label_ids['spyware']
            continue

        elif y_train[i] == 5:
            labelid[MASK_POS] = label_ids['adware']

        elif y_train[i] == 6:
            labelid[MASK_POS] = label_ids['dropper']
            continue

        elif y_train[i] == 7:
            labelid[MASK_POS] = label_ids['virus']

        # 除了mask对应的位置是病毒家族的token id，其他都是-1
        labelid[:MASK_POS] = [-1] * len(labelid[:MASK_POS])
        labelid[MASK_POS + 1:] = [-1] * len(labelid[MASK_POS + 1:])
        inputid[MASK_POS] = model_config.tokenizer.mask_token_id

        text.append(text_)
        Labelid.append(labelid)
        Inputid.append(inputid)

        attenmask.append(atten_mask)

    return Inputid, Labelid, attenmask


def trainer(config):
    log_filename = config.log_file_name
    model = Roberta_Model(roberta_path=config.pretrained_model_dir)

    # 查看本地是否存在相关模型（追加训练）

    if os.path.exists(config.model_acc_save_path):
        model.load_state_dict(torch.load(config.model_acc_save_path))
        print("从以下路径加载模型参数：", config.model_acc_save_path)
    else:
        print("未找到保存的参数。从头开始训练。")

    model = model.to(config.device)
    model.train()
    Inputid_train, Labelid_train, inputnmask_train = ProcessData(config.train_file_path,
                                                                 config.train_label_path)
    Inputid_dev, Labelid_dev, inputnmask_dev = ProcessData(config.val_file_path,
                                                           config.val_label_path)

    train_dataset = DataLoader(MyDataSet(Inputid_train, inputnmask_train, Labelid_train),
                               config.batch_size, True)
    valid_dataset = DataLoader(MyDataSet(Inputid_dev, inputnmask_dev, Labelid_dev),
                               config.batch_size,
                               True)

    train_data_num = len(Inputid_train)

    optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=1e-4)  # 使用Adam优化器
    loss_func = nn.CrossEntropyLoss(ignore_index=-1)
    EPOCH = config.epochs
    # 创建了一个学习率调度器。这里使用的是余弦退火调度器与预热步骤相结合
    schedule = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=len(train_dataset),
                                               num_training_steps=EPOCH * len(train_dataset))
    print("正在训练中。。。")
    totaltime = 0
    best_test_loss = float('inf')  # 初始化最佳测试集损失为正无穷大
    best_acc = 0  # 初始化最佳准确率为0
    MASK_POS = config.MASK_POS
    for epoch in range(EPOCH):

        # starttime_train = datetime.now()

        start = time.time()
        correct = 0
        train_loss_sum = 0
        model.train()

        for idx, (ids, att_mask, y) in enumerate(train_dataset):
            ids, att_mask, y = ids.to(config.device), att_mask.to(config.device), y.to(config.device)
            # 前向传播与损失计算
            out_train = model(ids, att_mask)
            # print(out_train.view(-1, tokenizer.vocab_size).shape, y.view(-1).shape)
            # out_train.view(-1, config.tokenizer.vocab_size)将输出变为二维张量，-1表示自动计算维度，列是vocab的大小
            loss = loss_func(out_train.view(-1, config.tokenizer.vocab_size), y.view(-1))
            # 反向传播与参数更新
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            schedule.step()
            train_loss_sum += loss.item()

            # 输出当前训练的 epoch、步数、损失和已经经过的时间，并将训练过程中的损失信息写入到 tensorboard 中。
            # 每50步输出一次训练信息
            if (idx + 1) % 50 == 0:
                print("Epoch {:04d} | Step {:06d}/{:06d} | Loss {:.4f} | Time {:.0f}".format(
                    epoch + 1, idx + 1, len(train_dataset), train_loss_sum / (idx + 1), time.time() - start))
                config.writer.add_scalar('loss/train_loss', train_loss_sum / (idx + 1), epoch)

            # 计算模型在训练集上的准确率

            truelabel = y[:, MASK_POS]
            out_train_mask = out_train[:, MASK_POS, :]
            predicted = torch.max(out_train_mask, 1)[1]
            correct += (predicted == truelabel).sum()
            correct = float(correct)

        train_accuracy = float(correct / train_data_num)
        mean_train_loss = train_loss_sum / (len(train_dataset))

        eval_loss_sum, valid_accuracy = evaluate_model(model, valid_dataset, loss_func, config)
        mean_valid_loss = eval_loss_sum / (len(valid_dataset))

        message = (f'Epoch [{epoch + 1}/{EPOCH}]: '
                   f'Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}, '
                   f'Train Accuracy: {train_accuracy:.4f}, Valid Accuracy: {valid_accuracy:.4f}, '
                   )
        print_and_log(message, log_filename)
        # 如果当前测试集损失更低，则保存模型参数
        if mean_valid_loss < best_test_loss:
            best_test_loss = mean_valid_loss
            torch.save(model.state_dict(), config.model_save_path)
            print('Saving model with loss {:.3f}...'.format(best_test_loss))

        # 如果当前验证集准确率更高，则保存模型参数

        if valid_accuracy > best_acc:
            best_acc = valid_accuracy
            torch.save(model.state_dict(), config.model_acc_save_path)
            print('Saving model with acc {:.3f}...'.format(best_acc))

        end = time.time()
        print("epoch {} duration:".format(epoch + 1), end - start)
        totaltime += end - start

        # 每个 epoch 结束后释放未使用的显存
        torch.cuda.empty_cache()

    print("total training time: ", totaltime)


def evaluate_model(model, test_dataset, loss_func, config):
    # 在训练中评估模型在验证集上的性能，返回损失和准确率
    eval_loss_sum = 0.0
    correct_test = 0
    MASK_POS = config.MASK_POS
    model.eval()
    with torch.no_grad():
        for ids, att, y in test_dataset:
            ids, att, y = ids.to(config.device), att.to(config.device), y.to(
                config.device)
            out_test = model(ids, att)
            loss_eval = loss_func(out_test.view(-1, config.tokenizer.vocab_size), y.view(-1))
            eval_loss_sum += loss_eval.item()
            ttruelabel = y[:, MASK_POS]
            tout_train_mask = out_test[:, MASK_POS, :]
            predicted_test = torch.max(tout_train_mask.data, 1)[1]
            correct_test += (predicted_test == ttruelabel).sum().item()

    acc_test = correct_test / len(test_dataset.dataset)
    return eval_loss_sum, acc_test


def print_and_log(message, file):
    print(message)
    print('正在写入日志文件：')
    with open(file, 'a') as log_file:
        log_file.write(message + '\n')


def evaluate_model_performance(config):
    # 不训练，直接评估模型在测试集上的性能。
    model = Roberta_Model(roberta_path=config.pretrained_model_dir)

    # 查看本地是否存在相关模型（否则无法评估）

    if os.path.exists(config.model_acc_save_path):
        model.load_state_dict(torch.load(config.model_acc_save_path))
        print("从以下路径加载模型参数：", config.model_acc_save_path)
    else:
        print("未找到保存的参数。无法进行评估测试")

    model = model.to(config.device)
    # 模型评估
    model.eval()

    Inputid_test, Labelid_test, inputnmask_test = ProcessData(config.test_file_path,
                                                              config.test_label_path)

    test_dataset = DataLoader(MyDataSet(Inputid_test, inputnmask_test, Labelid_test), config.batch_size,
                              True)

    predictions = []
    true_labels = []
    MASK_POS = config.MASK_POS
    with torch.no_grad():
        for input_ids, attention_mask, test_labels_processed in test_dataset:
            input_ids, attention_mask, test_labels_processed = input_ids.to(config.device), \
                                                               attention_mask.to(config.device), \
                                                               test_labels_processed.to(config.device)
            outputs = model(input_ids, attention_mask)
            predicted_label = torch.argmax(outputs[:, MASK_POS, :], dim=1)
            predictions.extend(predicted_label.cpu().numpy())
            true_labels.extend(test_labels_processed[:, MASK_POS].cpu().numpy())

    # 计算指标

    accuracy = accuracy_score(true_labels, predictions)
    precision = precision_score(true_labels, predictions, average='weighted')
    recall = recall_score(true_labels, predictions, average='weighted')
    f1 = f1_score(true_labels, predictions, average='weighted')

    message = (
        f'accuracy: {accuracy:.4f}, precision: {precision:.4f}, '
        f'recall: {recall:.4f}, f1: {f1:.4f}, '
    )
    print(message)

    return accuracy, precision, recall, f1


model_config = ModelConfig()

if __name__ == '__main__':
    model_config.MASK_POS = find_mask_position(prefix1, model_config.tokenizer)
    evaluate_model_performance(model_config)
